{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ns765\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ns765\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0005300084f90edc</td>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00054a5e18b50dd4</td>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0006f16e4e9f292e</td>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00070ef96486d6f9</td>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00078f8ce7eb276d</td>\n",
       "      <td>\"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000897889268bc93</td>\n",
       "      <td>REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0009801bd85e5806</td>\n",
       "      <td>The Mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0009eaea3325de8c</td>\n",
       "      <td>Don't mean to bother you \\n\\nI see that you're...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "0   0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1   000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2   000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3   0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4   0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "5   00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7   00031b1e95af7921  Your vandalism to the Matt Shirvington article...   \n",
       "8   00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...   \n",
       "9   00040093b2687caa  alignment on this subject and which are contra...   \n",
       "10  0005300084f90edc  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...   \n",
       "11  00054a5e18b50dd4  bbq \\n\\nbe a man and lets discuss it-maybe ove...   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "13  0006f16e4e9f292e  Before you start throwing accusations and warn...   \n",
       "14  00070ef96486d6f9  Oh, and the girl above started her arguments w...   \n",
       "15  00078f8ce7eb276d  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "17  000897889268bc93   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski   \n",
       "18  0009801bd85e5806  The Mitsurugi point made no sense - why not ar...   \n",
       "19  0009eaea3325de8c  Don't mean to bother you \\n\\nI see that you're...   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0       0             0        0       0       0              0  \n",
       "1       0             0        0       0       0              0  \n",
       "2       0             0        0       0       0              0  \n",
       "3       0             0        0       0       0              0  \n",
       "4       0             0        0       0       0              0  \n",
       "5       0             0        0       0       0              0  \n",
       "6       1             1        1       0       1              0  \n",
       "7       0             0        0       0       0              0  \n",
       "8       0             0        0       0       0              0  \n",
       "9       0             0        0       0       0              0  \n",
       "10      0             0        0       0       0              0  \n",
       "11      0             0        0       0       0              0  \n",
       "12      1             0        0       0       0              0  \n",
       "13      0             0        0       0       0              0  \n",
       "14      0             0        0       0       0              0  \n",
       "15      0             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "17      0             0        0       0       0              0  \n",
       "18      0             0        0       0       0              0  \n",
       "19      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "    \n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ','fick' ,' fux ', 'f\\*\\*',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck','fuk', 'wtf','fucck','f cking'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' asshole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'ass hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h','beetch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' transgender':\n",
    "        [\n",
    "            'transgender','trans gender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k','diick '\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bullshit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit','bs'\n",
    "        ],\n",
    "\n",
    "    ' homosexual':\n",
    "        [\n",
    "            'homo sexual','homosex'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots', 'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n",
    "        ],\n",
    "\n",
    "    ' shithole ':\n",
    "        [\n",
    "            'shythole','shit hole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumbass':\n",
    "        [\n",
    "            'dumb ass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' asshead':\n",
    "        [\n",
    "            'butthead', 'ass head'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            's3x', 'sexuality',\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' motherfucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker', 'mother fucker'\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Text Normalization\n",
    "\n",
    "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "  if is_lower:\n",
    "    text=text.lower()\n",
    "    \n",
    "  if remove_patterns_text:\n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "      for pat in patterns:\n",
    "        text=str(text).replace(pat, target)\n",
    "\n",
    "  if remove_repeat_text:\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "  text = str(text).replace(\"\\n\", \" \")\n",
    "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "  text = re.sub('[0-9]',\"\",text)\n",
    "  text = re.sub(\" +\", \" \", text)\n",
    "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "  return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_comment_text'] = train['comment_text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lemmatization\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "def lemma(text, lemmatization=True):\n",
    "  output=''\n",
    "  if lemmatization:\n",
    "    text=text.split(' ')\n",
    "    for word in text:\n",
    "      word1 = lemmatizer.lemmatize(word, pos = \"n\") #noun \n",
    "      word2 = lemmatizer.lemmatize(word1, pos = \"v\") #verb\n",
    "      word3 = lemmatizer.lemmatize(word2, pos = \"a\") #adjective\n",
    "      word4 = lemmatizer.lemmatize(word3, pos = \"r\") #adverb\n",
    "      output=output + \" \" + word4\n",
    "  else:\n",
    "    output=text\n",
    "  \n",
    "  return str(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_comment_text2'] = train['processed_comment_text'].apply(lambda x:lemma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry if the word nonsense wa offensive to you anyway i m not intend to write anything in the article wow they would jump on me for vandalism i m merely request that it be more encyclopedic so one can use it for school a a reference i have be to the selective breed page but it s almost a stub it point to animal breed which be a short messy article that give you no info there must be someone around with expertise in eugenics\n"
     ]
    }
   ],
   "source": [
    "train_comments = train['processed_comment_text2'].fillna(\"_na_\").values\n",
    "print (train_comments[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train[ ['toxic' , 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate'] ].values\n",
    "train_y[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 30000\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[286, 23, 1, 164, 746, 25, 1332, 3, 8, 493, 5, 73, 14, 882, 3, 110, 238, 11, 1, 18, 1330, 52, 53, 1570, 15, 37, 12, 224, 5, 73, 983, 207, 9, 10, 2, 66, 1275, 39, 46, 35, 42, 10, 12, 402, 4, 4, 141, 5, 16, 2, 3, 1, 3781, 2337, 24, 27, 10, 17, 661, 4, 1229, 10, 127, 3, 1397, 2337, 59, 2, 4, 623, 5776, 18, 9, 123, 8, 48, 463, 41, 274, 2, 171, 334, 22, 2885, 11, 10508]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_comments)\n",
    "train_comments_tokenized = tokenizer.texts_to_sequences(train_comments)\n",
    "print (train_comments_tokenized[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  286,    23,     1,   164,   746,    25,  1332,     3,     8,\n",
       "         493,     5,    73,    14,   882,     3,   110,   238,    11,\n",
       "           1,    18,  1330,    52,    53,  1570,    15,    37,    12,\n",
       "         224,     5,    73,   983,   207,     9,    10,     2,    66,\n",
       "        1275,    39,    46,    35,    42,    10,    12,   402,     4,\n",
       "           4,   141,     5,    16,     2,     3,     1,  3781,  2337,\n",
       "          24,    27,    10,    17,   661,     4,  1229,    10,   127,\n",
       "           3,  1397,  2337,    59,     2,     4,   623,  5776,    18,\n",
       "           9,   123,     8,    48,   463,    41,   274,     2,   171,\n",
       "         334,    22,  2885,    11, 10508,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = pad_sequences(train_comments_tokenized, maxlen=max_len, padding='post')\n",
    "train_X[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149254\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "print(len(word_index))\n",
    "# unique\n",
    "embedding_dim_fasttext = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_fasttext = {}\n",
    "f = open('E:\\wiki-news-300d-1M.vec\\wiki-news-300d-1M.vec', encoding='utf8')\n",
    "for line in f:\n",
    "    line.encode('utf-8').strip()\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()\n",
    "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM():\n",
    "    X_inp = Input( shape=(max_len,) )\n",
    "    \n",
    "    X = Embedding(max_words, 128)(X_inp)\n",
    "    \n",
    "    X = Bidirectional(LSTM(200, return_sequences=True))(X)\n",
    "    \n",
    "    X = Dropout(0.1)(X)\n",
    "    \n",
    "    X = LSTM(100, return_sequences=True)(X)\n",
    "    \n",
    "    X = GlobalAvgPool1D()(X)\n",
    "    \n",
    "    X = Dense(50, activation='relu')(X)\n",
    "    \n",
    "    X = Dropout(0.1)(X)\n",
    "    \n",
    "    X = Dense(6, activation='sigmoid')(X)\n",
    "    \n",
    "    model  = Model(inputs=X_inp, outputs=X)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ns765\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_model1 = BiLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ns765\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toxicity_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 128)          3840000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 400)          526400    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 400)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100, 100)          200400    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 100)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 306       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4572156 (17.44 MB)\n",
      "Trainable params: 4572156 (17.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "toxicity_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\ns765\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ns765\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 136/1995 [=>............................] - ETA: 23:14 - loss: 0.1744 - accuracy: 0.6247"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')  \n",
    "# for early stopping and preventing overfitting\n",
    "toxicity_model1.fit(train_X,train_y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[es])\n",
    "# data is divided into training and testing in 80:20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp, model):\n",
    "    inp_tokenized = tokenizer.texts_to_sequences([inp])\n",
    "    final_inp = pad_sequences(inp_tokenized, maxlen=max_len, padding='post')\n",
    "    print (model.predict(final_inp))\n",
    "    print('  toxic    ' , 'severe_toxic ' , 'obscene   ' , 'threat   ' , '  insult' , '  identity_hate')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"you are idiot\", toxicity_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"hey you have done a great job \", toxicity_model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toxicity_model1.save_weights(\"toxicity_model2_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('tokenizer.pk', 'wb') as tk:\n",
    "#     pickle.dump(tokenizer, tk, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
